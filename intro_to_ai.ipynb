{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0RGFSK0FdS8vYPwYwSZII",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SSSpock/skillspire/blob/main/intro_to_ai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.3blue1brown.com/lessons/neural-networks"
      ],
      "metadata": {
        "id": "NB-blBabMvM3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.3blue1brown.com/lessons/gradient-descent"
      ],
      "metadata": {
        "id": "rZwZqZpkNsEn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.3blue1brown.com/lessons/backpropagation"
      ],
      "metadata": {
        "id": "TbgQt4oSNt_E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1: Introduction to Neural Networks\n",
        "\n",
        "In this section, we will cover the basics of neural networks, their applications, structure, and activation functions.\n",
        "\n",
        "1.1 What are neural networks?\n",
        "\n",
        "Neural networks are a type of machine learning model inspired by the human brain. They consist of interconnected nodes (or neurons) organized in layers that work together to process and learn patterns from input data. Neural networks can learn complex patterns and representations, making them well-suited for tasks such as image recognition, natural language processing, and game playing.\n",
        "\n",
        "1.2 Applications of neural networks\n",
        "\n",
        "Neural networks have found widespread use in various domains, some of which include:\n",
        "\n",
        "- Image classification and object detection\n",
        "- Speech recognition and synthesis\n",
        "- Natural language processing (translation, sentiment analysis, question-answering)\n",
        "- Game playing (chess, Go)\n",
        "- Time-series prediction and anomaly detection\n",
        "\n",
        "1.3 Structure of neural networks\n",
        "\n",
        "A neural network typically consists of three types of layers:\n",
        "\n",
        "- Input layer: This layer receives input data and feeds it into the network.\n",
        "- Hidden layer(s): These are intermediate layers that process the input data, extracting patterns and features. A neural network can have multiple hidden layers, forming a \"deep\" neural network.\n",
        "- Output layer: This layer produces the final output or prediction from the processed data.\n",
        "\n",
        "Each layer contains nodes (neurons) connected to nodes in the adjacent layers. These connections have associated weights, which determine the strength of the connection between nodes. The data flows through the network, undergoing transformations at each node, ultimately producing the output.\n",
        "\n",
        "1.4 Activation functions\n",
        "\n",
        "Activation functions are used to introduce non-linearity in neural networks, allowing them to learn complex, non-linear patterns. These functions are applied to the weighted sum of the inputs at each node. Some popular activation functions include:\n",
        "\n",
        "- Sigmoid: The sigmoid function squashes the input value into the range (0, 1). It's commonly used in the output layer of binary classification problems.\n",
        "- Tanh: The hyperbolic tangent function is similar to the sigmoid function but has a range of (-1, 1). It's useful for problems where the output needs to be centered around 0.\n",
        "- ReLU (Rectified Linear Unit): The ReLU function returns the input value if it's positive and 0 if it's negative. ReLU is popular in hidden layers due to its simplicity and efficiency.\n",
        "\n",
        "In the next section, we will explore the concept of a perceptron, the simplest type of neural network."
      ],
      "metadata": {
        "id": "uhUPuccEKSY5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2: Perceptron\n",
        "\n",
        "In this section, we will discuss the concept of a perceptron, linear separability, the perceptron learning algorithm, its limitations, and an example of building a perceptron in Python.\n",
        "\n",
        "2.1 Concept of a perceptron\n",
        "\n",
        "A perceptron is the simplest type of neural network, consisting of a single layer with one or more input nodes and a single output node. It serves as the foundation for more complex neural network models. The perceptron receives input values (features), multiplies them by their corresponding weights, sums them up, and applies an activation function to produce the output.\n",
        "\n",
        "Mathematically, the output (y) of a perceptron can be represented as:\n",
        "\n",
        "y = f(w1*x1 + w2*x2 + ... + wn*xn + b)\n",
        "\n",
        "where:\n",
        "- x1, x2, ..., xn are the input features\n",
        "- w1, w2, ..., wn are the corresponding weights\n",
        "- b is the bias term\n",
        "- f is the activation function\n",
        "\n",
        "2.2 Linear separability\n",
        "\n",
        "A perceptron can only learn patterns that are linearly separable, meaning that the input data points can be separated into their respective classes using a straight line (or a hyperplane in higher dimensions). If the data is not linearly separable, the perceptron will fail to find a solution.\n",
        "\n",
        "2.3 Perceptron learning algorithm\n",
        "\n",
        "The perceptron learning algorithm is an iterative process used to find the optimal weights and bias for the perceptron. The algorithm updates the weights and bias based on the errors made in the predictions. The steps of the algorithm are:\n",
        "\n",
        "1. Initialize the weights and bias to small random values.\n",
        "2. For each input data point:\n",
        "   a. Calculate the output of the perceptron.\n",
        "   b. Update the weights and bias based on the error (difference between predicted and true output).\n",
        "3. Repeat steps 2 until the error converges or a maximum number of iterations is reached.\n",
        "\n",
        "2.4 Limitations of perceptrons\n",
        "\n",
        "Perceptrons have some limitations that hinder their ability to solve complex problems:\n",
        "\n",
        "- They can only learn linearly separable patterns.\n",
        "- They cannot represent complex, non-linear relationships between input features and output.\n",
        "- They do not support multiple output nodes, limiting their application to binary classification problems.\n",
        "\n",
        "2.5 Coding example: Building a perceptron in Python\n",
        "\n",
        "In this example, we will implement a simple perceptron using Python to learn the AND function, which is linearly separable.\n",
        "\n"
      ],
      "metadata": {
        "id": "Nl6Upz5fK66d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rhx0Xm0vJCEj",
        "outputId": "1841d86b-0f63-4a36-d343-72288edca3aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: [0 0], Output: 0\n",
            "Input: [0 1], Output: 0\n",
            "Input: [1 0], Output: 0\n",
            "Input: [1 1], Output: 1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Activation function\n",
        "def step_function(x):\n",
        "    return 1 if x >= 0 else 0\n",
        "\n",
        "# Perceptron training function\n",
        "def train_perceptron(X, y, learning_rate=0.1, epochs=100):\n",
        "    # Initialize weights and bias\n",
        "    weights = np.random.rand(X.shape[1])\n",
        "    bias = np.random.rand()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for xi, target in zip(X, y):\n",
        "            # Calculate the output\n",
        "            weighted_sum = np.dot(xi, weights) + bias\n",
        "            output = step_function(weighted_sum)\n",
        "\n",
        "            # Update weights and bias based on the error\n",
        "            error = target - output\n",
        "            weights += learning_rate * error * xi\n",
        "            bias += learning_rate * error\n",
        "\n",
        "    return weights, bias\n",
        "\n",
        "# Input data (AND function)\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([0, 0, 0, 1])\n",
        "\n",
        "# Train the perceptron\n",
        "weights, bias = train_perceptron(X, y)\n",
        "\n",
        "# Test the perceptron\n",
        "for xi in X:\n",
        "    weighted_sum = np.dot(xi, weights) + bias\n",
        "    output = step_function(weighted_sum)\n",
        "    print(f\"Input: {xi}, Output: {output}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this updated example, we first define the step function as our activation function, and then implement the train_perceptron function to train the perceptron. We use the AND function as our input data and train the perceptron with a learning rate of 0.1 and 100 epochs. After training, we test the perceptron on the same input data and print the results, which should correctly represent the AND function."
      ],
      "metadata": {
        "id": "ZTTNcNh-Lm-J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Section 3: Multi-Layer Perceptron (MLP)\n",
        "\n",
        "In this section, we will introduce the Multi-Layer Perceptron (MLP), discuss its layers, describe forward propagation, and provide a coding example of building an MLP in Python.\n",
        "\n",
        "3.1 Introduction to MLP\n",
        "\n",
        "A Multi-Layer Perceptron (MLP) is a type of neural network that consists of multiple layers of nodes (neurons) organized in an input layer, one or more hidden layers, and an output layer. MLPs can learn more complex patterns and representations compared to a single-layer perceptron, making them suitable for a wide range of tasks.\n",
        "\n",
        "3.2 Layers in an MLP\n",
        "\n",
        "An MLP has three types of layers:\n",
        "\n",
        "- Input layer: This layer receives input data and feeds it into the network.\n",
        "- Hidden layer(s): These are intermediate layers that process the input data, extracting patterns and features. An MLP can have multiple hidden layers, which allows it to learn more complex and abstract representations.\n",
        "- Output layer: This layer produces the final output or prediction from the processed data.\n",
        "\n",
        "3.3 Forward propagation\n",
        "\n",
        "Forward propagation is the process of passing input data through the MLP to obtain the final output. During forward propagation, the input data is transformed at each layer of the network by applying weights, biases, and activation functions. The steps of forward propagation are:\n",
        "\n",
        "1. Calculate the weighted sum of the inputs for each node in the first hidden layer.\n",
        "2. Apply the activation function to the weighted sum to obtain the output of the first hidden layer.\n",
        "3. Repeat steps 1 and 2 for each subsequent hidden layer, using the output of the previous layer as the input for the current layer.\n",
        "4. Calculate the weighted sum of the inputs for each node in the output layer.\n",
        "5. Apply the activation function (if applicable) to the weighted sum to obtain the final output.\n",
        "\n",
        "3.4 Coding example: Building an MLP in Python\n",
        "\n",
        "In this example, we will implement a simple MLP using Python to solve the XOR problem, which is not linearly separable.\n",
        "\n"
      ],
      "metadata": {
        "id": "S4WOAorSMx2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Activation function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Derivative of the sigmoid function\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# MLP architecture\n",
        "input_nodes = 2\n",
        "hidden_nodes = 2\n",
        "output_nodes = 1\n",
        "\n",
        "# Initialize weights and biases\n",
        "hidden_weights = np.random.rand(input_nodes, hidden_nodes)\n",
        "hidden_bias = np.random.rand(hidden_nodes)\n",
        "output_weights = np.random.rand(hidden_nodes, output_nodes)\n",
        "output_bias = np.random.rand(output_nodes)\n",
        "\n",
        "# Input data (XOR function)\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "# Train the MLP\n",
        "learning_rate = 0.5\n",
        "epochs = 5000\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Forward propagation\n",
        "    hidden_layer_input = np.dot(X, hidden_weights) + hidden_bias\n",
        "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
        "    output_layer_input = np.dot(hidden_layer_output, output_weights) + output_bias\n",
        "    output_layer_output = sigmoid(output_layer_input)\n",
        "\n",
        "    # Backpropagation\n",
        "    output_error = y - output_layer_output\n",
        "    output_delta = output_error * sigmoid_derivative(output_layer_output)\n",
        "\n",
        "    hidden_error = np.dot(output_delta, output_weights.T)\n",
        "    hidden_delta = hidden_error * sigmoid_derivative(hidden_layer_output)\n",
        "\n",
        "    # Update weights and biases\n",
        "    output_weights += np.dot(hidden_layer_output.T, output_delta) * learning_rate\n",
        "    output_bias += np.sum(output_delta, axis=0).reshape(-1) * learning_rate\n",
        "    hidden_weights += np.dot(X.T, hidden_delta) * learning_rate\n",
        "    hidden_bias += np.sum(hidden_delta, axis=0) * learning_rate\n",
        "\n",
        "# Test the MLP\n",
        "for xi in X:\n",
        "    hidden_layer_input = np.dot(xi, hidden_weights) + hidden_bias\n",
        "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
        "    output_layer_input = np.dot(hidden_layer_output, output_weights) + output_bias\n",
        "    output_layer_output = sigmoid(output_layer_input)\n",
        "    print(f\"Input: {xi}, Output: {output_layer_output}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHKMigo7NckS",
        "outputId": "ad78e0bf-027d-4c3d-ee37-0aa437ddf420"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: [0 0], Output: [0.03029795]\n",
            "Input: [0 1], Output: [0.97338029]\n",
            "Input: [1 0], Output: [0.97334647]\n",
            "Input: [1 1], Output: [0.02794162]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this updated example, we implement a simple MLP to solve the XOR problem. We first define the sigmoid activation function and its derivative, and then set up the MLP architecture with input, hidden, and output nodes. We initialize weights and biases, and use the XOR function as our input data.\n",
        "\n",
        "We train the MLP using forward propagation and backpropagation for 5000 epochs with a learning rate of 0.5. After training, we test the MLP on the same input data and print the results, which should correctly represent the XOR function."
      ],
      "metadata": {
        "id": "HZ37DL3aPDA1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 4: Gradient Descent\n",
        "\n",
        "In this section, we will discuss gradient descent as an optimization technique, its types, the learning rate, and loss functions. We will also provide a visualization of gradient descent using Plotly.\n",
        "\n",
        "4.1 What is gradient descent?\n",
        "\n",
        "Gradient descent is an optimization algorithm used to minimize a function (usually a loss function) by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. In the context of neural networks, gradient descent is used to find the optimal weights and biases that minimize the loss function.\n",
        "\n",
        "4.2 Types of gradient descent\n",
        "\n",
        "There are three main types of gradient descent:\n",
        "\n",
        "1. Batch gradient descent: Updates the weights and biases using the entire dataset at each iteration.\n",
        "2. Stochastic gradient descent (SGD): Updates the weights and biases using a single data point at each iteration.\n",
        "3. Mini-batch gradient descent: Updates the weights and biases using a small subset (batch) of the dataset at each iteration.\n",
        "\n",
        "4.3 Learning rate\n",
        "\n",
        "The learning rate is a hyperparameter that controls how much the weights and biases are updated at each iteration. A small learning rate results in slow convergence but better fine-tuning, while a large learning rate results in faster convergence but may cause overshooting and instability.\n",
        "\n",
        "4.4 Loss functions\n",
        "\n",
        "A loss function is a measure of how well a neural network is performing in terms of its predictions. It quantifies the difference between the predicted output and the actual output (ground truth). Common loss functions include mean squared error (MSE), cross-entropy loss, and mean absolute error (MAE).\n"
      ],
      "metadata": {
        "id": "5hkDCxWCNQ2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Quadratic function and its derivative\n",
        "def func(x):\n",
        "    return x**2\n",
        "\n",
        "def derivative(x):\n",
        "    return 2 * x\n",
        "\n",
        "# Gradient descent algorithm\n",
        "def gradient_descent(x_start, learning_rate, epochs):\n",
        "    x_values = [x_start]\n",
        "    y_values = [func(x_start)]\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        x_start = x_start - learning_rate * derivative(x_start)\n",
        "        x_values.append(x_start)\n",
        "        y_values.append(func(x_start))\n",
        "\n",
        "    return x_values, y_values\n",
        "\n",
        "# Parameters\n",
        "x_start = -7\n",
        "learning_rate = 0.1\n",
        "epochs = 30\n",
        "\n",
        "x_values, y_values = gradient_descent(x_start, learning_rate, epochs)\n",
        "\n",
        "# Plot the function and gradient descent steps\n",
        "x = np.linspace(-10, 10, 1000)\n",
        "y = func(x)\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(x=x, y=y, mode='lines', name='Function'))\n",
        "fig.add_trace(go.Scatter(x=x_values, y=y_values, mode='markers', name='Gradient Descent Steps'))\n",
        "\n",
        "fig.update_layout(title='Gradient Descent Visualization', xaxis_title='x', yaxis_title='y', showlegend=True)\n",
        "\n",
        "fig.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "Pq1itdsxQH-a",
        "outputId": "4d2efee8-06f8-4990-dc5e-e9f99bae54a4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.18.2.min.js\"></script>                <div id=\"f6b75f48-aa3a-47d9-99c7-62522d859cc5\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"f6b75f48-aa3a-47d9-99c7-62522d859cc5\")) {                    Plotly.newPlot(                        \"f6b75f48-aa3a-47d9-99c7-62522d859cc5\",                        [{\"mode\":\"lines\",\"name\":\"Function\",\"x\":[-10.0,-9.97997997997998,-9.95995995995996,-9.93993993993994,-9.91991991991992,-9.8998998998999,-9.87987987987988,-9.85985985985986,-9.83983983983984,-9.81981981981982,-9.7997997997998,-9.77977977977978,-9.75975975975976,-9.73973973973974,-9.71971971971972,-9.6996996996997,-9.67967967967968,-9.65965965965966,-9.63963963963964,-9.61961961961962,-9.5995995995996,-9.57957957957958,-9.55955955955956,-9.53953953953954,-9.51951951951952,-9.4994994994995,-9.47947947947948,-9.45945945945946,-9.43943943943944,-9.41941941941942,-9.3993993993994,-9.37937937937938,-9.35935935935936,-9.33933933933934,-9.31931931931932,-9.2992992992993,-9.27927927927928,-9.25925925925926,-9.23923923923924,-9.21921921921922,-9.1991991991992,-9.17917917917918,-9.15915915915916,-9.13913913913914,-9.11911911911912,-9.0990990990991,-9.07907907907908,-9.05905905905906,-9.03903903903904,-9.01901901901902,-8.998998998999,-8.97897897897898,-8.95895895895896,-8.93893893893894,-8.91891891891892,-8.8988988988989,-8.87887887887888,-8.85885885885886,-8.83883883883884,-8.81881881881882,-8.7987987987988,-8.77877877877878,-8.75875875875876,-8.73873873873874,-8.71871871871872,-8.6986986986987,-8.67867867867868,-8.65865865865866,-8.63863863863864,-8.618618618618619,-8.598598598598599,-8.578578578578579,-8.558558558558559,-8.538538538538539,-8.518518518518519,-8.498498498498499,-8.478478478478479,-8.458458458458459,-8.438438438438439,-8.418418418418419,-8.398398398398399,-8.378378378378379,-8.358358358358359,-8.338338338338339,-8.318318318318319,-8.298298298298299,-8.278278278278279,-8.258258258258259,-8.238238238238239,-8.218218218218219,-8.198198198198199,-8.178178178178179,-8.158158158158159,-8.138138138138139,-8.118118118118119,-8.098098098098099,-8.078078078078079,-8.058058058058059,-8.038038038038039,-8.018018018018019,-7.997997997997998,-7.977977977977978,-7.957957957957958,-7.937937937937938,-7.917917917917918,-7.897897897897898,-7.877877877877878,-7.857857857857858,-7.837837837837838,-7.817817817817818,-7.797797797797798,-7.777777777777778,-7.757757757757758,-7.737737737737738,-7.717717717717718,-7.697697697697698,-7.677677677677678,-7.657657657657658,-7.637637637637638,-7.617617617617618,-7.597597597597598,-7.5775775775775776,-7.5575575575575575,-7.5375375375375375,-7.5175175175175175,-7.4974974974974975,-7.4774774774774775,-7.4574574574574575,-7.4374374374374375,-7.4174174174174174,-7.397397397397397,-7.377377377377377,-7.357357357357357,-7.337337337337337,-7.317317317317317,-7.297297297297297,-7.277277277277277,-7.257257257257257,-7.237237237237237,-7.217217217217217,-7.197197197197197,-7.177177177177177,-7.157157157157157,-7.137137137137137,-7.117117117117117,-7.097097097097097,-7.077077077077077,-7.057057057057057,-7.037037037037037,-7.017017017017017,-6.996996996996997,-6.976976976976977,-6.956956956956957,-6.936936936936937,-6.916916916916917,-6.896896896896897,-6.876876876876877,-6.856856856856857,-6.836836836836837,-6.816816816816817,-6.796796796796797,-6.776776776776776,-6.756756756756756,-6.736736736736736,-6.716716716716716,-6.696696696696696,-6.676676676676676,-6.656656656656656,-6.636636636636636,-6.616616616616616,-6.596596596596596,-6.576576576576576,-6.556556556556556,-6.536536536536536,-6.516516516516516,-6.496496496496496,-6.476476476476476,-6.456456456456456,-6.436436436436436,-6.416416416416416,-6.396396396396396,-6.376376376376376,-6.356356356356356,-6.336336336336336,-6.316316316316316,-6.296296296296296,-6.276276276276276,-6.256256256256256,-6.236236236236236,-6.216216216216216,-6.196196196196196,-6.176176176176176,-6.156156156156156,-6.136136136136136,-6.116116116116116,-6.096096096096096,-6.076076076076076,-6.056056056056056,-6.036036036036036,-6.016016016016016,-5.995995995995996,-5.975975975975976,-5.955955955955956,-5.935935935935936,-5.915915915915916,-5.895895895895896,-5.875875875875876,-5.8558558558558556,-5.8358358358358355,-5.8158158158158155,-5.7957957957957955,-5.7757757757757755,-5.7557557557557555,-5.7357357357357355,-5.7157157157157155,-5.6956956956956954,-5.675675675675675,-5.655655655655655,-5.635635635635635,-5.615615615615615,-5.595595595595595,-5.575575575575575,-5.555555555555555,-5.535535535535535,-5.515515515515515,-5.495495495495495,-5.475475475475475,-5.455455455455455,-5.435435435435435,-5.415415415415415,-5.395395395395395,-5.375375375375375,-5.355355355355355,-5.335335335335335,-5.315315315315315,-5.295295295295295,-5.275275275275275,-5.255255255255255,-5.235235235235235,-5.215215215215215,-5.195195195195195,-5.175175175175175,-5.155155155155155,-5.135135135135135,-5.115115115115115,-5.095095095095095,-5.075075075075075,-5.055055055055055,-5.035035035035035,-5.015015015015015,-4.994994994994995,-4.974974974974975,-4.954954954954955,-4.934934934934935,-4.914914914914915,-4.894894894894895,-4.874874874874875,-4.854854854854855,-4.834834834834835,-4.814814814814815,-4.794794794794795,-4.774774774774775,-4.754754754754755,-4.734734734734735,-4.714714714714715,-4.694694694694695,-4.674674674674675,-4.654654654654655,-4.634634634634635,-4.614614614614615,-4.594594594594595,-4.574574574574575,-4.554554554554555,-4.534534534534535,-4.514514514514515,-4.494494494494495,-4.474474474474475,-4.454454454454455,-4.434434434434435,-4.414414414414415,-4.394394394394395,-4.374374374374375,-4.354354354354355,-4.334334334334335,-4.314314314314315,-4.2942942942942945,-4.2742742742742745,-4.2542542542542545,-4.2342342342342345,-4.2142142142142145,-4.1941941941941945,-4.1741741741741745,-4.1541541541541545,-4.134134134134134,-4.114114114114114,-4.094094094094094,-4.074074074074074,-4.054054054054054,-4.034034034034034,-4.014014014014014,-3.9939939939939944,-3.9739739739739743,-3.9539539539539543,-3.9339339339339343,-3.9139139139139143,-3.8938938938938943,-3.8738738738738743,-3.8538538538538543,-3.8338338338338342,-3.8138138138138142,-3.7937937937937942,-3.773773773773774,-3.753753753753754,-3.733733733733734,-3.713713713713714,-3.693693693693694,-3.673673673673674,-3.653653653653654,-3.633633633633634,-3.613613613613614,-3.593593593593594,-3.573573573573573,-3.553553553553553,-3.533533533533533,-3.513513513513513,-3.493493493493493,-3.473473473473473,-3.453453453453453,-3.433433433433433,-3.413413413413413,-3.393393393393393,-3.373373373373373,-3.353353353353353,-3.333333333333333,-3.313313313313313,-3.293293293293293,-3.273273273273273,-3.253253253253253,-3.233233233233233,-3.213213213213213,-3.193193193193193,-3.173173173173173,-3.153153153153153,-3.133133133133133,-3.113113113113113,-3.093093093093093,-3.073073073073073,-3.053053053053053,-3.033033033033033,-3.013013013013013,-2.992992992992993,-2.972972972972973,-2.952952952952953,-2.932932932932933,-2.9129129129129128,-2.8928928928928928,-2.8728728728728727,-2.8528528528528527,-2.8328328328328327,-2.8128128128128127,-2.7927927927927927,-2.7727727727727727,-2.7527527527527527,-2.7327327327327327,-2.7127127127127126,-2.6926926926926926,-2.6726726726726726,-2.6526526526526526,-2.6326326326326326,-2.6126126126126126,-2.5925925925925926,-2.5725725725725725,-2.5525525525525525,-2.5325325325325325,-2.5125125125125125,-2.4924924924924925,-2.4724724724724725,-2.4524524524524525,-2.4324324324324325,-2.4124124124124124,-2.3923923923923924,-2.3723723723723724,-2.3523523523523524,-2.3323323323323324,-2.3123123123123124,-2.2922922922922924,-2.2722722722722724,-2.2522522522522523,-2.2322322322322323,-2.2122122122122123,-2.1921921921921923,-2.1721721721721723,-2.1521521521521523,-2.1321321321321323,-2.1121121121121122,-2.0920920920920922,-2.0720720720720722,-2.052052052052052,-2.032032032032032,-2.012012012012012,-1.9919919919919913,-1.9719719719719713,-1.9519519519519513,-1.9319319319319312,-1.9119119119119112,-1.8918918918918912,-1.8718718718718712,-1.8518518518518512,-1.8318318318318312,-1.8118118118118112,-1.7917917917917912,-1.7717717717717711,-1.7517517517517511,-1.7317317317317311,-1.711711711711711,-1.691691691691691,-1.671671671671671,-1.651651651651651,-1.631631631631631,-1.611611611611611,-1.591591591591591,-1.571571571571571,-1.551551551551551,-1.531531531531531,-1.511511511511511,-1.491491491491491,-1.471471471471471,-1.451451451451451,-1.431431431431431,-1.411411411411411,-1.391391391391391,-1.3713713713713709,-1.3513513513513509,-1.3313313313313309,-1.3113113113113108,-1.2912912912912908,-1.2712712712712708,-1.2512512512512508,-1.2312312312312308,-1.2112112112112108,-1.1911911911911908,-1.1711711711711708,-1.1511511511511507,-1.1311311311311307,-1.1111111111111107,-1.0910910910910907,-1.0710710710710707,-1.0510510510510507,-1.0310310310310307,-1.0110110110110107,-0.9909909909909906,-0.9709709709709706,-0.9509509509509506,-0.9309309309309306,-0.9109109109109106,-0.8908908908908906,-0.8708708708708706,-0.8508508508508505,-0.8308308308308305,-0.8108108108108105,-0.7907907907907905,-0.7707707707707705,-0.7507507507507505,-0.7307307307307305,-0.7107107107107105,-0.6906906906906904,-0.6706706706706704,-0.6506506506506504,-0.6306306306306304,-0.6106106106106104,-0.5905905905905904,-0.5705705705705704,-0.5505505505505504,-0.5305305305305303,-0.5105105105105103,-0.4904904904904903,-0.4704704704704703,-0.4504504504504503,-0.4304304304304303,-0.41041041041041026,-0.39039039039039025,-0.37037037037037024,-0.3503503503503502,-0.3303303303303302,-0.3103103103103102,-0.2902902902902902,-0.2702702702702702,-0.25025025025025016,-0.23023023023023015,-0.21021021021021014,-0.19019019019019012,-0.1701701701701701,-0.1501501501501501,-0.13013013013013008,-0.11011011011011007,-0.09009009009009006,-0.07007007007007005,-0.05005005005005003,-0.03003003003003002,-0.010010010010010006,0.010010010010010006,0.03003003003003002,0.05005005005005003,0.07007007007007005,0.09009009009009006,0.11011011011011007,0.13013013013013008,0.1501501501501501,0.1701701701701701,0.19019019019019012,0.21021021021021014,0.23023023023023015,0.25025025025025016,0.2702702702702702,0.2902902902902902,0.3103103103103102,0.3303303303303302,0.3503503503503502,0.37037037037037024,0.39039039039039025,0.41041041041041026,0.4304304304304303,0.4504504504504503,0.4704704704704703,0.4904904904904903,0.5105105105105103,0.5305305305305303,0.5505505505505504,0.5705705705705704,0.5905905905905904,0.6106106106106104,0.6306306306306304,0.6506506506506504,0.6706706706706704,0.6906906906906904,0.7107107107107105,0.7307307307307305,0.7507507507507505,0.7707707707707705,0.7907907907907905,0.8108108108108105,0.8308308308308305,0.8508508508508505,0.8708708708708706,0.8908908908908906,0.9109109109109106,0.9309309309309306,0.9509509509509506,0.9709709709709706,0.9909909909909906,1.0110110110110107,1.0310310310310307,1.0510510510510507,1.0710710710710707,1.0910910910910907,1.1111111111111107,1.1311311311311307,1.1511511511511507,1.1711711711711708,1.1911911911911908,1.2112112112112108,1.2312312312312308,1.2512512512512508,1.2712712712712708,1.2912912912912908,1.3113113113113108,1.3313313313313309,1.3513513513513509,1.3713713713713709,1.391391391391391,1.411411411411411,1.431431431431431,1.451451451451451,1.471471471471471,1.491491491491491,1.511511511511511,1.531531531531531,1.551551551551551,1.571571571571571,1.591591591591591,1.611611611611611,1.631631631631631,1.651651651651651,1.671671671671671,1.691691691691691,1.711711711711711,1.7317317317317311,1.7517517517517511,1.7717717717717711,1.7917917917917912,1.8118118118118112,1.8318318318318312,1.8518518518518512,1.8718718718718712,1.8918918918918912,1.9119119119119112,1.9319319319319312,1.9519519519519513,1.9719719719719713,1.9919919919919913,2.0120120120120113,2.0320320320320313,2.0520520520520513,2.0720720720720713,2.0920920920920913,2.1121121121121114,2.1321321321321314,2.1521521521521514,2.1721721721721714,2.1921921921921914,2.2122122122122114,2.2322322322322314,2.2522522522522515,2.2722722722722715,2.2922922922922915,2.3123123123123115,2.3323323323323315,2.3523523523523515,2.3723723723723715,2.3923923923923915,2.4124124124124116,2.4324324324324316,2.4524524524524516,2.4724724724724716,2.4924924924924916,2.5125125125125116,2.5325325325325316,2.5525525525525516,2.5725725725725717,2.5925925925925917,2.6126126126126117,2.6326326326326317,2.6526526526526517,2.6726726726726717,2.6926926926926917,2.7127127127127117,2.7327327327327318,2.7527527527527518,2.772772772772772,2.792792792792792,2.812812812812812,2.8328328328328336,2.8528528528528536,2.8728728728728736,2.8928928928928936,2.9129129129129137,2.9329329329329337,2.9529529529529537,2.9729729729729737,2.9929929929929937,3.0130130130130137,3.0330330330330337,3.0530530530530537,3.0730730730730738,3.0930930930930938,3.113113113113114,3.133133133133134,3.153153153153154,3.173173173173174,3.193193193193194,3.213213213213214,3.233233233233234,3.253253253253254,3.273273273273274,3.293293293293294,3.313313313313314,3.333333333333334,3.353353353353354,3.373373373373374,3.393393393393394,3.413413413413414,3.433433433433434,3.453453453453454,3.473473473473474,3.493493493493494,3.513513513513514,3.533533533533534,3.553553553553554,3.573573573573574,3.593593593593594,3.613613613613614,3.633633633633634,3.653653653653654,3.673673673673674,3.693693693693694,3.713713713713714,3.733733733733734,3.753753753753754,3.773773773773774,3.7937937937937942,3.8138138138138142,3.8338338338338342,3.8538538538538543,3.8738738738738743,3.8938938938938943,3.9139139139139143,3.9339339339339343,3.9539539539539543,3.9739739739739743,3.9939939939939944,4.014014014014014,4.034034034034034,4.054054054054054,4.074074074074074,4.094094094094094,4.114114114114114,4.134134134134134,4.1541541541541545,4.1741741741741745,4.1941941941941945,4.2142142142142145,4.2342342342342345,4.2542542542542545,4.2742742742742745,4.2942942942942945,4.314314314314315,4.334334334334335,4.354354354354355,4.374374374374375,4.394394394394395,4.414414414414415,4.434434434434435,4.454454454454455,4.474474474474475,4.494494494494495,4.514514514514515,4.534534534534535,4.554554554554555,4.574574574574575,4.594594594594595,4.614614614614615,4.634634634634635,4.654654654654655,4.674674674674675,4.694694694694695,4.714714714714715,4.734734734734735,4.754754754754755,4.774774774774775,4.794794794794795,4.814814814814815,4.834834834834835,4.854854854854855,4.874874874874875,4.894894894894895,4.914914914914915,4.934934934934935,4.954954954954955,4.974974974974975,4.994994994994995,5.015015015015015,5.035035035035035,5.055055055055055,5.075075075075075,5.095095095095095,5.115115115115115,5.135135135135135,5.155155155155155,5.175175175175175,5.195195195195195,5.215215215215215,5.235235235235235,5.255255255255255,5.275275275275275,5.295295295295295,5.315315315315315,5.335335335335335,5.355355355355355,5.375375375375375,5.395395395395395,5.415415415415415,5.435435435435435,5.455455455455455,5.475475475475475,5.495495495495495,5.515515515515515,5.535535535535535,5.555555555555555,5.575575575575575,5.595595595595595,5.615615615615615,5.635635635635635,5.655655655655655,5.675675675675675,5.6956956956956954,5.7157157157157155,5.7357357357357355,5.7557557557557555,5.7757757757757755,5.7957957957957955,5.8158158158158155,5.8358358358358355,5.8558558558558556,5.875875875875876,5.895895895895896,5.915915915915916,5.935935935935936,5.955955955955956,5.975975975975976,5.995995995995996,6.016016016016017,6.0360360360360374,6.0560560560560575,6.0760760760760775,6.0960960960960975,6.1161161161161175,6.1361361361361375,6.1561561561561575,6.1761761761761775,6.1961961961961975,6.216216216216218,6.236236236236238,6.256256256256258,6.276276276276278,6.296296296296298,6.316316316316318,6.336336336336338,6.356356356356358,6.376376376376378,6.396396396396398,6.416416416416418,6.436436436436438,6.456456456456458,6.476476476476478,6.496496496496498,6.516516516516518,6.536536536536538,6.556556556556558,6.576576576576578,6.596596596596598,6.616616616616618,6.636636636636638,6.656656656656658,6.676676676676678,6.696696696696698,6.716716716716718,6.736736736736738,6.756756756756758,6.776776776776778,6.796796796796798,6.816816816816818,6.836836836836838,6.856856856856858,6.876876876876878,6.896896896896898,6.916916916916918,6.936936936936938,6.956956956956958,6.976976976976978,6.996996996996998,7.017017017017018,7.037037037037038,7.057057057057058,7.077077077077078,7.097097097097098,7.117117117117118,7.137137137137138,7.157157157157158,7.177177177177178,7.197197197197198,7.217217217217218,7.237237237237238,7.257257257257258,7.277277277277278,7.297297297297298,7.317317317317318,7.337337337337338,7.357357357357358,7.377377377377378,7.397397397397398,7.417417417417418,7.437437437437438,7.457457457457458,7.477477477477478,7.497497497497498,7.517517517517518,7.537537537537538,7.557557557557558,7.577577577577578,7.5975975975975985,7.6176176176176185,7.6376376376376385,7.6576576576576585,7.6776776776776785,7.6976976976976985,7.7177177177177185,7.7377377377377385,7.7577577577577586,7.777777777777779,7.797797797797799,7.817817817817819,7.837837837837839,7.857857857857859,7.877877877877879,7.897897897897899,7.917917917917919,7.937937937937939,7.957957957957959,7.977977977977979,7.997997997997999,8.018018018018019,8.038038038038039,8.058058058058059,8.078078078078079,8.098098098098099,8.118118118118119,8.138138138138139,8.158158158158159,8.178178178178179,8.198198198198199,8.218218218218219,8.238238238238239,8.258258258258259,8.278278278278279,8.298298298298299,8.318318318318319,8.338338338338339,8.358358358358359,8.378378378378379,8.398398398398399,8.418418418418419,8.438438438438439,8.458458458458459,8.478478478478479,8.498498498498499,8.518518518518519,8.538538538538539,8.558558558558559,8.578578578578579,8.598598598598599,8.618618618618619,8.63863863863864,8.65865865865866,8.67867867867868,8.6986986986987,8.71871871871872,8.73873873873874,8.75875875875876,8.77877877877878,8.7987987987988,8.81881881881882,8.83883883883884,8.85885885885886,8.87887887887888,8.8988988988989,8.91891891891892,8.93893893893894,8.95895895895896,8.97897897897898,8.998998998999,9.01901901901902,9.03903903903904,9.05905905905906,9.07907907907908,9.0990990990991,9.11911911911912,9.13913913913914,9.15915915915916,9.17917917917918,9.1991991991992,9.21921921921922,9.23923923923924,9.25925925925926,9.27927927927928,9.2992992992993,9.31931931931932,9.33933933933934,9.35935935935936,9.37937937937938,9.3993993993994,9.41941941941942,9.43943943943944,9.45945945945946,9.47947947947948,9.4994994994995,9.51951951951952,9.53953953953954,9.55955955955956,9.57957957957958,9.5995995995996,9.61961961961962,9.63963963963964,9.65965965965966,9.67967967967968,9.6996996996997,9.71971971971972,9.73973973973974,9.75975975975976,9.77977977977978,9.7997997997998,9.81981981981982,9.83983983983984,9.85985985985986,9.87987987987988,9.8998998998999,9.91991991991992,9.93993993993994,9.95995995995996,9.97997997997998,10.0],\"y\":[100.0,99.6000004008012,99.20080240400561,98.80240600961322,98.40481121762403,98.00801802803805,97.61202644085527,97.2168364560757,96.82244807369932,96.42886129372616,96.0360761161562,95.64409254098943,95.25291056822589,94.86253019786554,94.47295142990839,94.08417426435444,93.69619870120371,93.30902474045618,92.92265238211185,92.53708162617072,92.1523124726328,91.76834492149808,91.38517897276657,91.00281462643825,90.62125188251315,90.24049074099125,89.86053120187255,89.48137326515705,89.10301693084476,88.72546219893567,88.34870906942979,87.97275754232712,87.59760761762765,87.22325929533137,86.8497125754383,86.47696745794845,86.10502394286179,85.73388203017834,85.36354171989808,84.99400301202104,84.6252659065472,84.25733040347656,83.89019650280912,83.52386420454489,83.15833350868387,82.79360441522604,82.42967692417143,82.06655103552,81.7042267492718,81.3427040654268,80.98198298398499,80.6220635049464,80.262945628311,79.9046293540788,79.54711468224983,79.19040161282405,78.83449014580147,78.47938028118209,78.12507201896592,77.77156535915296,77.4188603017432,77.06695684673663,76.71585499413328,76.36555474393313,76.01605609613618,75.66735905074245,75.3194636077519,74.97236976716457,74.62607752898045,74.28058689319951,73.9358978598218,73.59201042884727,73.24892460027596,72.90664037410785,72.56515775034295,72.22447672898124,71.88459731002274,71.54551949346745,71.20724327931536,70.86976866756648,70.53309565822079,70.19722425127831,69.86215444673904,69.52788624460297,69.19441964487011,68.86175464754044,68.52989125261398,68.19882946009074,67.86856926997068,67.53911068225383,67.21045369694019,66.88259831402975,66.55554453352252,66.2292923554185,65.90384177971767,65.57919280642004,65.25534543552563,64.93229966703441,64.6100555009464,64.2886129372616,63.96797197597998,63.64813261710158,63.32909486062639,63.0108587065544,62.69342415488561,62.37679120562003,62.060959858757656,61.74593011429848,61.431701972242514,61.11827543258975,60.80565049534018,60.49382716049382,60.18280542805067,59.87258529801072,59.563166770373975,59.25454984514043,58.946734522310095,58.639720801882966,58.33350868385903,58.02809816823831,57.72348925502079,57.41968194420647,57.11667623579535,56.81447212978745,56.51306962618274,56.212468724981235,55.91266942618294,55.613671729787846,55.315475635795956,55.01808114420727,54.72148825502179,54.425696968239514,54.13070728386044,53.83651920188457,53.5431327223119,53.25054784514244,52.95876457037618,52.66778289801313,52.37760282805328,52.08822436049663,51.799647495343194,51.51187223259296,51.22489857224592,50.93872651430209,50.65335605876147,50.368787205624045,50.08501995488983,49.802054306558816,49.519890260631,49.2385278171064,48.957966975984995,48.678207737266796,48.399250100951804,48.12109406704002,47.84373963553143,47.56718680642605,47.291435579723874,47.0164859554249,46.74233793352913,46.46899151403656,46.1964466969472,45.92470348226103,45.65376186997808,45.38362186009833,45.114283452621784,44.84574664754844,44.57801144487831,44.31107784461137,44.04494584674764,43.779615451287114,43.51508665822979,43.251359467575675,42.98843387932477,42.72630989347705,42.46498751003255,42.20446672899125,41.94474755035315,41.685829974118256,41.427714000286564,41.17039962885808,40.9138868598328,40.65817569321072,40.403266128991845,40.14915816717618,39.89585180776371,39.64334705075445,39.391643896148395,39.14074234394554,38.89064239414589,38.641344046749445,38.392847301756206,38.145152159166166,37.898258618979334,37.6521666811957,37.40687634581528,37.16238761283806,36.91870048226404,36.67581495409323,36.43373102832562,36.192448704961215,35.951967984000014,35.71228886544201,35.47341134928722,35.23533543553563,34.998061124187245,34.761588415242066,34.52591730870009,34.291047804561316,34.056979902825745,33.82371360349338,33.59124890656422,33.35958581203826,33.128724319915506,32.89866443019596,32.669406142879616,32.440949457966475,32.213294375456535,31.9864408953498,31.760389017646272,31.535138742345946,31.310690069448825,31.08704299895491,30.864197530864196,30.642153665176686,30.42091140189238,30.20047074101128,29.98083168253338,29.76199422645869,29.5439583727872,29.326724121518915,29.110291472653834,28.894660426191955,28.679830982133282,28.465803140477814,28.25257690122555,28.040152264376488,27.82852922993063,27.617707797887977,27.407687968248528,27.198469741012282,26.99005311617924,26.782438093749406,26.57562467372277,26.36961285609934,26.164402640879118,25.959994028062095,25.75638701764828,25.553581609637664,25.351577804030256,25.150375600826052,24.94997500002505,24.750376001627252,24.55157860563266,24.35358281204127,24.156388620853086,23.959996032068105,23.764405045686328,23.569615661707754,23.375627880132384,23.18244170096022,22.99005712419126,22.798474149825502,22.607692777862948,22.4177130083036,22.228534841147454,22.040158276394514,21.852583314044775,21.665809954098243,21.479838196554915,21.294668041414788,21.110299488677867,20.92673253834415,20.74396719041364,20.56200344488633,20.380841301762224,20.200480761041323,20.020921822723626,19.842164486809132,19.664208753297846,19.48705462218976,19.310702093484878,19.135151167183203,18.96040184328473,18.786454121789458,18.613308002697394,18.440963486008535,18.269420571722875,18.098679259840424,17.928739550361176,17.759601443285128,17.591264938612287,17.42373003634265,17.256996736476218,17.09106503901299,16.925934943952964,16.761606451296142,16.59807956104253,16.435354273192115,16.273430587744905,16.1123085047009,15.9519880240601,15.792469145822501,15.633751869988108,15.475836196556921,15.318722125528936,15.162409656904154,15.006898790682577,14.852189526864205,14.698281865449037,14.54517580643707,14.39287134982831,14.241368495622753,14.0906672438204,13.940767594421251,13.791669547425306,13.643373102832566,13.495878260643028,13.349185020856696,13.203293383473568,13.058203348493642,12.913914915916921,12.770428085743399,12.627742857973086,12.485859232605977,12.344777209642071,12.204496789081372,12.065017970923874,11.926340755169582,11.788465141818493,11.651391130870609,11.515118722325928,11.379647916184451,11.244978712446178,11.111111111111109,10.978045112179245,10.845780715650584,10.714317921525128,10.583656729802874,10.453797140483825,10.32473915356798,10.19648276905534,10.069027986945903,9.94237480723967,9.816523229936642,9.691473255036817,9.567224882540197,9.443778112446779,9.321132944756567,9.199289379469558,9.078247416585754,8.958007056105153,8.838568298027756,8.719931142353564,8.602095589082575,8.485061638214791,8.36882928975021,8.253398543688833,8.138769400030661,8.024941858775692,7.911915919923928,7.799691583475367,7.68826884943001,7.577647717787857,7.467828188548909,7.358810261713164,7.250593937280623,7.143179215251287,7.036566095625155,6.930754578402226,6.825744663582501,6.72153635116598,6.618129641152664,6.515524533542552,6.413721028335643,6.312719125531938,6.212518825131438,6.113120127134141,6.014523031540048,5.91672753834916,5.819733647561476,5.723541359176995,5.6281506731957185,5.533561589617646,5.439774108442777,5.3467882296711124,5.254603953302652,5.163221279337396,5.072640207775343,4.982860738616495,4.893882871860851,4.80570660750841,4.718331945559173,4.631758886013141,4.545987428870312,4.461017574130688,4.376849321794268,4.293482671861051,4.210917624331039,4.12915417920423,4.048192336480626,3.9680320961602216,3.888673458243025,3.8101164227290325,3.732360989618244,3.65540715891066,3.5792549306062793,3.503904304705103,3.4293552812071306,3.3556078601123622,3.2826620414207976,3.2105178251324373,3.1391752112472813,3.0686341997653286,2.99889479068658,2.929956984011036,2.8618207797386956,2.794486177869559,2.727953178403627,2.6622217813408984,2.597291986681374,2.533163794425054,2.4698372045719377,2.4073122171220254,2.345588832075317,2.2846670494318126,2.2245468691915122,2.165228291354416,2.1067113159205237,2.048995942889835,1.992082172262351,1.9359700040380707,1.8806594382169943,1.8261504747991222,1.7724431137844538,1.7195373551729896,1.6674331989647293,1.616130645159673,1.5656296937578207,1.5159303447591725,1.4670325981637282,1.418936453971488,1.3716419121824517,1.3251489727966195,1.2794576358139913,1.234567901234567,1.1904797690583468,1.1471932392853306,1.1047083119155183,1.0630249869489101,1.022143264385506,0.9820631442253057,0.9427846264683095,0.9043077111145172,0.8666323981639291,0.8297586876165449,0.7936865794723647,0.7584160737313885,0.7239471703936163,0.6902798694590482,0.657414170927684,0.6253500747995238,0.5940875810745676,0.5636266897528155,0.5339674008342673,0.5051097143189232,0.47705363020678304,0.44979914849784686,0.4233462691921147,0.3976949922895866,0.3728453177902625,0.34879724569414233,0.3255507760012262,0.3031059087115141,0.28146264382500596,0.26062098134170186,0.24058092126160177,0.22134246358470566,0.20290560831101356,0.18527035544052547,0.1684367049732414,0.1524046569091613,0.13717421124828522,0.12274536799061316,0.10911812713614508,0.09629248868488101,0.08426845263682095,0.07304601899196489,0.06262518775031282,0.05300595891186478,0.04418833247662073,0.03617230844458069,0.028957886815744655,0.02254506759011262,0.01693385076768459,0.012124236348460564,0.008116224332440542,0.004909814719624526,0.002505007510012513,0.0009018027036045048,0.00010020030040050053,0.00010020030040050053,0.0009018027036045048,0.002505007510012513,0.004909814719624526,0.008116224332440542,0.012124236348460564,0.01693385076768459,0.02254506759011262,0.028957886815744655,0.03617230844458069,0.04418833247662073,0.05300595891186478,0.06262518775031282,0.07304601899196489,0.08426845263682095,0.09629248868488101,0.10911812713614508,0.12274536799061316,0.13717421124828522,0.1524046569091613,0.1684367049732414,0.18527035544052547,0.20290560831101356,0.22134246358470566,0.24058092126160177,0.26062098134170186,0.28146264382500596,0.3031059087115141,0.3255507760012262,0.34879724569414233,0.3728453177902625,0.3976949922895866,0.4233462691921147,0.44979914849784686,0.47705363020678304,0.5051097143189232,0.5339674008342673,0.5636266897528155,0.5940875810745676,0.6253500747995238,0.657414170927684,0.6902798694590482,0.7239471703936163,0.7584160737313885,0.7936865794723647,0.8297586876165449,0.8666323981639291,0.9043077111145172,0.9427846264683095,0.9820631442253057,1.022143264385506,1.0630249869489101,1.1047083119155183,1.1471932392853306,1.1904797690583468,1.234567901234567,1.2794576358139913,1.3251489727966195,1.3716419121824517,1.418936453971488,1.4670325981637282,1.5159303447591725,1.5656296937578207,1.616130645159673,1.6674331989647293,1.7195373551729896,1.7724431137844538,1.8261504747991222,1.8806594382169943,1.9359700040380707,1.992082172262351,2.048995942889835,2.1067113159205237,2.165228291354416,2.2245468691915122,2.2846670494318126,2.345588832075317,2.4073122171220254,2.4698372045719377,2.533163794425054,2.597291986681374,2.6622217813408984,2.727953178403627,2.794486177869559,2.8618207797386956,2.929956984011036,2.99889479068658,3.0686341997653286,3.1391752112472813,3.2105178251324373,3.2826620414207976,3.3556078601123622,3.4293552812071306,3.503904304705103,3.5792549306062793,3.65540715891066,3.732360989618244,3.8101164227290325,3.888673458243025,3.9680320961602216,4.048192336480622,4.129154179204226,4.210917624331035,4.293482671861047,4.376849321794263,4.461017574130684,4.545987428870308,4.631758886013137,4.71833194555917,4.805706607508406,4.893882871860846,4.982860738616491,5.072640207775339,5.1632212793373915,5.254603953302648,5.346788229671109,5.439774108442773,5.5335615896176416,5.628150673195714,5.72354135917699,5.819733647561471,5.916727538349155,6.014523031540044,6.113120127134136,6.212518825131434,6.312719125531934,6.4137210283356385,6.515524533542547,6.61812964115266,6.721536351165976,6.825744663582497,6.9307545784022215,7.03656609562515,7.1431792152512825,7.250593937280619,7.35881026171316,7.467828188548904,7.577647717787853,7.688268849430005,7.7996915834753615,7.911915919923922,8.024941858775698,8.138769400030666,8.253398543688839,8.368829289750215,8.485061638214797,8.60209558908258,8.71993114235357,8.838568298027761,8.958007056105158,9.078247416585759,9.199289379469564,9.321132944756572,9.443778112446786,9.567224882540202,9.691473255036822,9.816523229936648,9.942374807239677,10.069027986945908,10.196482769055345,10.324739153567986,10.45379714048383,10.58365672980288,10.714317921525133,10.845780715650589,10.97804511217925,11.111111111111114,11.244978712446184,11.379647916184457,11.515118722325933,11.651391130870614,11.788465141818499,11.926340755169587,12.065017970923881,12.204496789081377,12.344777209642078,12.485859232605984,12.627742857973091,12.770428085743404,12.913914915916921,13.058203348493642,13.203293383473568,13.349185020856696,13.495878260643028,13.643373102832566,13.791669547425306,13.940767594421251,14.0906672438204,14.241368495622753,14.39287134982831,14.54517580643707,14.698281865449037,14.852189526864205,15.006898790682577,15.162409656904154,15.318722125528936,15.475836196556921,15.633751869988108,15.792469145822501,15.9519880240601,16.1123085047009,16.273430587744905,16.435354273192115,16.59807956104253,16.761606451296142,16.925934943952964,17.09106503901299,17.256996736476218,17.42373003634265,17.591264938612287,17.759601443285128,17.928739550361176,18.098679259840424,18.269420571722875,18.440963486008535,18.613308002697394,18.786454121789458,18.96040184328473,19.135151167183203,19.310702093484878,19.48705462218976,19.664208753297846,19.842164486809132,20.020921822723626,20.200480761041323,20.380841301762224,20.56200344488633,20.74396719041364,20.92673253834415,21.110299488677867,21.294668041414788,21.479838196554915,21.665809954098243,21.852583314044775,22.040158276394514,22.228534841147454,22.4177130083036,22.607692777862948,22.798474149825502,22.99005712419126,23.18244170096022,23.375627880132384,23.569615661707754,23.764405045686328,23.959996032068105,24.156388620853086,24.35358281204127,24.55157860563266,24.750376001627252,24.94997500002505,25.150375600826052,25.351577804030256,25.553581609637664,25.75638701764828,25.959994028062095,26.164402640879118,26.36961285609934,26.57562467372277,26.782438093749406,26.99005311617924,27.198469741012282,27.407687968248528,27.617707797887977,27.82852922993063,28.040152264376488,28.25257690122555,28.465803140477814,28.679830982133282,28.894660426191955,29.110291472653834,29.326724121518915,29.5439583727872,29.76199422645869,29.98083168253338,30.20047074101128,30.42091140189238,30.642153665176686,30.864197530864196,31.08704299895491,31.310690069448825,31.535138742345946,31.760389017646272,31.9864408953498,32.213294375456535,32.440949457966475,32.669406142879616,32.89866443019596,33.128724319915506,33.35958581203826,33.59124890656422,33.82371360349338,34.056979902825745,34.291047804561316,34.52591730870009,34.761588415242066,34.998061124187245,35.23533543553563,35.47341134928722,35.71228886544201,35.951967984000014,36.19244870496124,36.43373102832564,36.67581495409325,36.918700482264065,37.16238761283808,37.4068763458153,37.65216668119573,37.898258618979355,38.14515215916619,38.39284730175623,38.64134404674947,38.890642394145914,39.14074234394556,39.391643896148416,39.64334705075447,39.895851807763734,40.1491581671762,40.40326612899187,40.658175693210744,40.91388685983282,41.17039962885811,41.42771400028659,41.68582997411828,41.94474755035317,42.20446672899127,42.46498751003257,42.72630989347708,42.98843387932479,43.251359467575696,43.51508665822982,43.779615451287135,44.044945846747666,44.3110778446114,44.57801144487833,44.84574664754847,45.114283452621805,45.38362186009835,45.653761869978105,45.92470348226106,46.19644669694721,46.468991514036574,46.74233793352914,47.01648595542491,47.29143557972388,47.56718680642606,47.84373963553144,48.121094067040026,48.39925010095182,48.67820773726681,48.95796697598501,49.23852781710641,49.519890260631016,49.80205430655882,50.08501995488984,50.36878720562406,50.65335605876148,50.938726514302104,51.224898572245934,51.51187223259297,51.79964749534321,52.088224360496646,52.37760282805329,52.667782898013144,52.958764570376196,53.25054784514246,53.54313272231192,53.836519201884585,54.13070728386045,54.42569696823953,54.721488255021804,55.01808114420729,55.31547563579597,55.61367172978786,55.91266942618295,56.21246872498125,56.513069626182755,56.81447212978746,57.11667623579537,57.41968194420648,57.7234892550208,58.02809816823832,58.333508683859044,58.63972080188298,58.94673452231011,59.254549845140446,59.56316677037399,59.87258529801073,60.182805428050685,60.49382716049384,60.805650495340196,61.11827543258976,61.43170197224252,61.745930114298496,62.06095985875767,62.376791205620044,62.693424154885626,63.010858706554416,63.329094860626405,63.648132617101595,63.96797197597999,64.2886129372616,64.6100555009464,64.93229966703441,65.25534543552563,65.57919280642004,65.90384177971767,66.2292923554185,66.55554453352252,66.88259831402975,67.21045369694019,67.53911068225383,67.86856926997068,68.19882946009074,68.52989125261398,68.86175464754044,69.19441964487011,69.52788624460297,69.86215444673904,70.19722425127831,70.53309565822079,70.86976866756648,71.20724327931536,71.54551949346745,71.88459731002274,72.22447672898124,72.56515775034295,72.90664037410785,73.24892460027596,73.59201042884727,73.9358978598218,74.28058689319951,74.62607752898045,74.97236976716457,75.3194636077519,75.66735905074245,76.01605609613618,76.36555474393313,76.71585499413328,77.06695684673663,77.4188603017432,77.77156535915296,78.12507201896592,78.47938028118209,78.83449014580147,79.19040161282405,79.54711468224983,79.9046293540788,80.262945628311,80.6220635049464,80.98198298398499,81.3427040654268,81.7042267492718,82.06655103552,82.42967692417143,82.79360441522604,83.15833350868387,83.52386420454489,83.89019650280912,84.25733040347656,84.6252659065472,84.99400301202104,85.36354171989808,85.73388203017834,86.10502394286179,86.47696745794845,86.8497125754383,87.22325929533137,87.59760761762765,87.97275754232712,88.34870906942979,88.72546219893567,89.10301693084476,89.48137326515705,89.86053120187255,90.24049074099125,90.62125188251315,91.00281462643825,91.38517897276657,91.76834492149808,92.1523124726328,92.53708162617072,92.92265238211185,93.30902474045618,93.69619870120371,94.08417426435444,94.47295142990839,94.86253019786554,95.25291056822589,95.64409254098943,96.0360761161562,96.42886129372616,96.82244807369932,97.2168364560757,97.61202644085527,98.00801802803805,98.40481121762403,98.80240600961322,99.20080240400561,99.6000004008012,100.0],\"type\":\"scatter\"},{\"mode\":\"markers\",\"name\":\"Gradient Descent Steps\",\"x\":[-7,-5.6,-4.4799999999999995,-3.5839999999999996,-2.8671999999999995,-2.29376,-1.8350079999999998,-1.4680063999999997,-1.1744051199999999,-0.9395240959999999,-0.7516192767999998,-0.6012954214399999,-0.48103633715199995,-0.38482906972159997,-0.30786325577727996,-0.24629060462182398,-0.19703248369745918,-0.15762598695796734,-0.12610078956637388,-0.1008806316530991,-0.08070450532247928,-0.06456360425798342,-0.05165088340638674,-0.04132070672510939,-0.03305656538008751,-0.02644525230407001,-0.02115620184325601,-0.01692496147460481,-0.013539969179683847,-0.010831975343747077,-0.008665580274997661],\"y\":[49,31.359999999999996,20.070399999999996,12.845055999999998,8.220835839999998,5.261334937599999,3.367254360063999,2.1550427904409593,1.3792273858822142,0.882705526964617,0.5649315372573548,0.3615561838447071,0.23139595766061258,0.14809341290279204,0.09477978425778691,0.060659061924983625,0.03882179963198952,0.02484595176447329,0.015901409129262908,0.01017690184272826,0.006513217179346086,0.004168458994781495,0.002667813756660157,0.0017074008042625,0.0010927365147280002,0.0006993513694259203,0.000447584876432589,0.000286454320916857,0.00018333076538678845,0.0001173316898475446,7.509228150242854e-05],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Gradient Descent Visualization\"},\"xaxis\":{\"title\":{\"text\":\"x\"}},\"yaxis\":{\"title\":{\"text\":\"y\"}},\"showlegend\":true},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('f6b75f48-aa3a-47d9-99c7-62522d859cc5');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}